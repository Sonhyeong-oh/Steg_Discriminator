import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from CNN_Disc import ResNet, Dir4LaplacianBlur, Dir8LaplacianBlur
import cv2
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Dict, Any
import random
from torchvision import transforms
import os
from PIL import Image, ImageChops
from PPOTrainer import PPOTrainer
import time
import json



# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def to_tensor_from_bgr(np_img, device):
    # np_img: (H, W, 3), BGR, uint8
    rgb_img = np_img[..., ::-1].copy()  # BGR â†’ RGB
    tensor = torch.from_numpy(rgb_img).permute(2, 0, 1).float() / 255.0  # (3, H, W)
    return tensor.unsqueeze(0).to(device)  # (1, 3, H, W)


def pil_to_cv2(pil_img):
    arr = np.array(pil_img)
    return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)


# -------------------------------
# 1. Hard Attention Mask Generator
# -------------------------------
def generate_attention_mask(height, width, center, radius=100, device='cuda'):
    """
    í•˜ë“œ ì›í˜• ì–´í…ì…˜ ë§ˆìŠ¤í¬ (ê°’ì€ 1 ë˜ëŠ” 0)
    """
    if center.ndim == 2:
        center = center[0]
    cy, cx = center[0].item(), center[1].item()

    # ì¢Œí‘œ ê·¸ë¦¬ë“œ ìƒì„±
    y = torch.arange(0, height, device=device).view(-1, 1).float()
    x = torch.arange(0, width, device=device).view(1, -1).float()
    dist = torch.sqrt((x - cx)**2 + (y - cy)**2)

    # í•˜ë“œ ë§ˆìŠ¤í¬: ì› ë‚´ë¶€ëŠ” 1, ì™¸ë¶€ëŠ” 0
    mask = (dist <= radius).float()

    return mask.unsqueeze(0)  # (1, H, W)


class SteganalysisEnv:
    """
    ìŠ¤í…Œê°€ë¶„ì„ì„ ìœ„í•œ ê°•í™”í•™ìŠµ í™˜ê²½
    ë…¼ë¬¸ì˜ "Self-Seeking Steganalysis" í™˜ê²½ êµ¬í˜„
    """
    
    def __init__(self, 
                 image_dataset,
                 labels_dataset, 
                 discriminant_model,
                 max_episode_steps=5,           # ë…¼ë¬¸ì—ì„œ T=5
                 image_size=(300, 300),
                 initial_center_range=0.3):
        """
        Args:
            image_dataset: ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (torch.Tensor or list)
            labels_dataset: ë¼ë²¨ ë°ì´í„°ì…‹ (0: cover, 1: stego)
            discriminant_model: ë¯¸ë¦¬ í•™ìŠµëœ ìŠ¤í…Œê°€ë¶„ì„ ëª¨ë¸
            max_episode_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
            image_size: ì´ë¯¸ì§€ í¬ê¸°
            initial_center_range: ì´ˆê¸° ì¤‘ì‹¬ì  ë²”ìœ„ (0.3 = ì¤‘ì•™ 30% ì˜ì—­)
        """
        self.image_dataset = image_dataset
        self.labels_dataset = labels_dataset
        self.discriminant_model = discriminant_model
        self.max_episode_steps = max_episode_steps
        self.image_size = image_size
        self.initial_center_range = initial_center_range
        
        # í™˜ê²½ ìƒíƒœ
        self.current_image = None
        self.current_label = None
        self.current_center = None
        self.step_count = 0
        self.episode_reward = 0
        
        # ì„±ëŠ¥ ì¶”ì 
        self.attention_history = []
        self.reward_history = []
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = next(discriminant_model.parameters()).device
        
        # í•„í„° ì„¤ì •
        self.filter = Dir8LaplacianBlur().to(self.device)  # í™˜ê²½ ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ìƒì„±
        self.blur = Dir4LaplacianBlur().to(self.device)
        
    def reset(self, image_idx=None):
        """
        í™˜ê²½ ì´ˆê¸°í™”
        Returns:
            state: ì´ˆê¸° ìƒíƒœ (image, center)
        """
        # ëœë¤í•˜ê²Œ ì´ë¯¸ì§€ ì„ íƒ (ë˜ëŠ” ì§€ì •ëœ ì¸ë±ìŠ¤)

        if image_idx is None:
            idx = random.randint(0, len(self.image_dataset) - 1)
        else:
            idx = image_idx
            
        self.current_image = self.image_dataset[idx]
        self.current_label = self.labels_dataset[idx]
        
        # í…ì„œ ë³€í™˜
        if isinstance(self.current_image, np.ndarray):
            self.current_image = torch.from_numpy(self.current_image).float()

        # (H, W) âœ (1, 1, H, W)
        if self.current_image.ndim == 2:
            self.current_image = self.current_image.unsqueeze(0).unsqueeze(0)

        # (H, W, 3) âœ (1, 3, H, W)
        if self.current_image.ndim == 3 and self.current_image.shape[-1] == 3:
            self.current_image = self.current_image.permute(2, 0, 1).unsqueeze(0)

        # (3, H, W) or (1, H, W) âœ (1, C, H, W)
        if self.current_image.ndim == 3 and self.current_image.shape[0] in [1, 3]:
            self.current_image = self.current_image.unsqueeze(0)

        # (1, 1, H, W) âœ (1, 3, H, W)
        if self.current_image.ndim == 4 and self.current_image.shape[1] == 1:
            self.current_image = self.current_image.repeat(1, 3, 1, 1)

        # ìµœì¢… í™•ì¸
        assert self.current_image.shape[1] == 3, f"ì´ë¯¸ì§€ëŠ” RGB 3ì±„ë„ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ shape: {self.current_image.shape}"
        
        # ì¤‘ì‹¬ì  ë¨¼ì € ì„¤ì •
        H, W = self.image_size
        center_range_h = int(H * self.initial_center_range)
        center_range_w = int(W * self.initial_center_range)

        center_y = random.randint(H//2 - center_range_h//2, H//2 + center_range_h//2)
        center_x = random.randint(W//2 - center_range_w//2, W//2 + center_range_w//2)

        self.current_center = torch.tensor([[center_y, center_x]], dtype=torch.long)

        # ìƒíƒœ ì´ˆê¸°í™”
        self.step_count = 0
        self.episode_reward = 0
        self.attention_history = [self.current_center.clone()]
        self.reward_history = []
        
        return self._get_state()
    
    def step(self, action):
        """
        í™˜ê²½ì—ì„œ í•œ ìŠ¤í… ì§„í–‰
        Args:
            action: í–‰ë™ (0~15, 16ë°©í–¥ ì›€ì§ì„)
        Returns:
            next_state: ë‹¤ìŒ ìƒíƒœ
            reward: ë³´ìƒ
            done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
            info: ì¶”ê°€ ì •ë³´
        """
        self.step_count += 1
        
        # ì•¡ì…˜ì— ë”°ë¼ ì¤‘ì‹¬ì  ì´ë™
        self.current_center = self._move_center(self.current_center, action)
        
        # ë³´ìƒ ê³„ì‚°
        reward = self._compute_reward()
        self.episode_reward += reward
        self.reward_history.append(reward)
        
        # ì£¼ì˜ ìœ„ì¹˜ ê¸°ë¡
        self.attention_history.append(self.current_center.clone())
        
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        done = self.step_count >= self.max_episode_steps
        
        # ì¶”ê°€ ì •ë³´
        info = {
            'step': self.step_count,
            'episode_reward': self.episode_reward,
            'center': self.current_center.cpu().numpy(),
            'classification_accuracy': self._get_classification_accuracy()
        }
        
        return self._get_state(), reward, done, info
    
    def _get_state(self):
        """
        í˜„ì¬ ìƒíƒœ ë°˜í™˜
        Returns:
            dict: {'image': tensor, 'center': tensor}
        """
        return {
            'image': self.current_image,
            'center': self.current_center,
            'step': self.step_count
        }
    
    def _move_center(self, current_center, action):
        """
        ì•¡ì…˜ì— ë”°ë¼ ì¤‘ì‹¬ì  ì´ë™
        """
        # 16ë°©í–¥ ì›€ì§ì„ ì •ì˜ (8ë°©í–¥ Ã— 2ê±°ë¦¬)
        action_space_size = 32  # í”½ì…€ ë‹¨ìœ„ ì´ë™ ê±°ë¦¬
        angles = np.linspace(0, 2*np.pi, 9)[:-1]  # 8 directions
        
        action_vectors = []
        for angle in angles:
            # Short distance
            dx_short = int(action_space_size * 0.5 * np.cos(angle))
            dy_short = int(action_space_size * 0.5 * np.sin(angle))
            action_vectors.append((dy_short, dx_short))
            
            # Long distance  
            dx_long = int(action_space_size * np.cos(angle))
            dy_long = int(action_space_size * np.sin(angle))
            action_vectors.append((dy_long, dx_long))
        
        # í˜„ì¬ ì¤‘ì‹¬ì 
        center_y, center_x = current_center[0, 0].item(), current_center[0, 1].item()
        
        # ì•¡ì…˜ ì ìš©
        if action < len(action_vectors):
            dy, dx = action_vectors[action]
            new_y = max(0, min(self.image_size[0]-1, center_y + dy))
            new_x = max(0, min(self.image_size[1]-1, center_x + dx))
        else:
            # ì˜ëª»ëœ ì•¡ì…˜ì¸ ê²½ìš° í˜„ì¬ ìœ„ì¹˜ ìœ ì§€
            new_y, new_x = center_y, center_x
        
        return torch.tensor([[new_y, new_x]], dtype=torch.long)
    
    def _compute_reward(self):
        """
        ë³´ìƒ ê³„ì‚° (BCE ê¸°ë°˜ ì‹¤ìˆ˜í˜• ë³´ìƒ + ì—”íŠ¸ë¡œí”¼ + ê²½ê³„ íŒ¨ë„í‹°)
        """
        with torch.no_grad():
            # í˜„ì¬ ì£¼ì˜ ì˜ì—­ì— ë§ˆìŠ¤í¬ ì ìš©
            masked_image = self._apply_dual_filter_mask(self.current_image, self.current_center)
            masked_image = masked_image.to(self.device)

            # ìŠ¤í…Œê°€ë¶„ì„ ì˜ˆì¸¡
            logits = self.discriminant_model(masked_image)  # shape: (1,) or (B,)
            logits = logits.view(-1)  # ensure shape is (1,)

            label_tensor = torch.tensor([self.current_label], dtype=torch.float32, device=self.device)

            # BCEWithLogitsLossë¡œ ì‹¤ìˆ˜í˜• loss ê³„ì‚°
            loss = F.binary_cross_entropy_with_logits(logits, label_tensor)
            accuracy_reward = 1.0 / (1.0 + loss.item())  # ë‚®ì€ loss â†’ ë†’ì€ ë³´ìƒ

            # í™•ë¥  ê³„ì‚° (sigmoid)
            probs = torch.sigmoid(logits)
            entropy = - (probs * torch.log(probs + 1e-8) + (1 - probs) * torch.log(1 - probs + 1e-8))
            entropy_reward = 1.0 / (1.0 + entropy.item())

            # ê²½ê³„ í˜ë„í‹°
            boundary_penalty = self._compute_boundary_penalty()

            # ì´ ë³´ìƒ ê³„ì‚°
            total_reward = accuracy_reward + 0.1 * entropy_reward - 0.05 * boundary_penalty

        

        return total_reward
    
    def _apply_dual_filter_mask(self, image, center, radius=100, return_numpy=False, return_bgr=False):
        """
        ì–´í…ì…˜ ì¤‘ì‹¬ ì£¼ë³€ì—ëŠ” self.filter (ì˜ˆ: 8ë°©í–¥ Laplacian), 
        ê·¸ ì™¸ì—ëŠ” self.blur (ì˜ˆ: 4ë°©í–¥ Laplacian)ë¥¼ ì ìš©.
    
        Args:
            image (torch.Tensor): (B, 3, H, W)
            center (torch.Tensor): (B, 2)
            radius (int): attention ì˜ì—­ ë°˜ì§€ë¦„
            return_numpy (bool): Trueì´ë©´ NumPy ì´ë¯¸ì§€(RGB ë˜ëŠ” BGR) ë°˜í™˜
            return_bgr (bool): Trueì´ë©´ BGR NumPy ì´ë¯¸ì§€ ë°˜í™˜ (OpenCVìš©)
    
        Returns:
            torch.Tensor or np.ndarray: í•„í„° ì ìš© ì´ë¯¸ì§€
        """
        B, C, H, W = image.shape
        output_images = []
    
        for b in range(B):
            center_b = center[b]
    
            # ğŸ¯ í•˜ë“œ ë§ˆìŠ¤í¬ ìƒì„±
            mask = generate_attention_mask(H, W, center_b, radius, device=self.device)  # (1, H, W)
            mask = mask.to(dtype=image.dtype, device=self.device).expand(C, H, W).unsqueeze(0)  # (1, 3, H, W)
            inverted_mask = 1 - mask
    
            image_b = image[b:b+1]  # (1, 3, H, W)
    
            # í•„í„°ì™€ ë¸”ëŸ¬ ì ìš©
            focused = self.filter(image_b)   # (1, 3, H, W)
            blurred = self.blur(image_b)     # (1, 3, H, W)
    
            # ğŸ¯ ë§ˆìŠ¤í‚¹ í›„ ë³‘í•© (GPU í…ì„œë¼ë¦¬ ì—°ì‚°)
            combined = mask * focused + inverted_mask * blurred  # (1, 3, H, W)
            output_images.append(combined)
    
        final_tensor = torch.cat(output_images, dim=0)  # (B, 3, H, W)
    
        if return_numpy:
            # í•˜ë‚˜ì˜ ë°°ì¹˜ë§Œ ë³€í™˜
            img_tensor = final_tensor[0].detach().cpu().permute(1, 2, 0).numpy()  # (H, W, C)
            img_tensor *= 255.0
    
            if return_bgr:
                img_tensor = cv2.cvtColor(img_tensor, cv2.COLOR_RGB2BGR)
    
            return img_tensor
    
        return final_tensor

    def _compute_boundary_penalty(self):
        """
        ê²½ê³„ í˜ë„í‹° ê³„ì‚° (ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ë¡œ ê°ˆìˆ˜ë¡ í˜ë„í‹°)
        """
        center_y, center_x = self.current_center[0, 0].item(), self.current_center[0, 1].item()
        H, W = self.image_size
        
        # ê²½ê³„ë¡œë¶€í„°ì˜ ìµœì†Œ ê±°ë¦¬
        min_dist_to_boundary = min(center_y, center_x, H - center_y, W - center_x)
        
        # ê²½ê³„ ê·¼ì²˜ (50í”½ì…€ ì´ë‚´)ë©´ í˜ë„í‹°
        boundary_threshold = 50
        if min_dist_to_boundary < boundary_threshold:
            penalty = (boundary_threshold - min_dist_to_boundary) / boundary_threshold
        else:
            penalty = 0
            
        return penalty
    
    def _get_classification_accuracy(self):
        """
        í˜„ì¬ ìœ„ì¹˜ì—ì„œì˜ ë¶„ë¥˜ ì •í™•ë„
        """
        with torch.no_grad():          
            masked_image = self._apply_dual_filter_mask(self.current_image, self.current_center).to(self.device)
            # ì´ë¯¸ì§€ ì €ì¥
            logits = self.discriminant_model(masked_image)
            prob = torch.sigmoid(logits).item()  # scalar
            pred_label = int(prob > 0.5)
            accuracy = float(pred_label == self.current_label)

        return accuracy
    
    def render(self, save_path=None):
        """
        í•„í„° ë° ì–´í…ì…˜ ì ìš© ì´ë¯¸ì§€ë“¤ì„ ì‹œê°í™” ë° ì €ì¥.
        """

        def tensor_to_numpy_for_cv2(tensor):
            """(1, 3, H, W) â†’ (H, W, 3), uint8, BGR"""
            if tensor.is_cuda:
                tensor = tensor.cpu()
            np_img = tensor.detach().numpy()[0]  # (3, H, W)
            np_img = np.transpose(np_img, (1, 2, 0))  # (H, W, 3)
            np_img = (np_img * 255)
            return np_img

        # 1. ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
        og_img_bgr = tensor_to_numpy_for_cv2(self.current_image)
        cv2.imwrite("og_image.png", og_img_bgr)

        # 2. í•„í„° ì ìš© ì´ë¯¸ì§€ ìƒì„±
        with torch.no_grad():
            filtered_tensor = self._apply_dual_filter_mask(self.current_image, self.current_center)

        vis_img_bgr = tensor_to_numpy_for_cv2(filtered_tensor)
        cv2.imwrite("attention_visualization.png", vis_img_bgr)

        # 3. ì‹œê° ê°•ì¡° (ë¹¨ê°„ ì›)
        sector_img = vis_img_bgr.copy()
        center_y, center_x = self.current_center[0, 0].item(), self.current_center[0, 1].item()
        cv2.circle(sector_img, (center_x, center_y), 100, (0, 0, 255), 2)
        cv2.imwrite("attention_sector.png", sector_img)

        # 4. ì–´í…ì…˜ ì´ë™ ê²½ë¡œ ì‹œê°í™”
        route_img = vis_img_bgr.copy()
        cv2.circle(route_img, (center_x, center_y), 100, (0, 0, 255), 2)

        if len(self.attention_history) > 1:
            for i in range(1, len(self.attention_history)):
                prev = self.attention_history[i - 1][0]
                curr = self.attention_history[i][0]
                pt1 = (prev[1].item(), prev[0].item())
                pt2 = (curr[1].item(), curr[0].item())
                cv2.arrowedLine(route_img, pt1, pt2, (255, 0, 0), 2, tipLength=0.3)

        cv2.imwrite("attention_route.png", route_img)

        # 5. ë°˜í™˜ ë˜ëŠ” ì €ì¥
        if save_path:
            cv2.imwrite(save_path, sector_img)

        return sector_img  # (H, W, 3), BGR, uint8
    
    def get_attention_summary(self):
        """
        ì£¼ì˜ ì˜ì—­ ìš”ì•½ ì •ë³´ (SoAFR - Summary of Attention-Focused Regions)
        """
        return {
            'attention_centers': [center.cpu().numpy() for center in self.attention_history],
            'rewards': self.reward_history,
            'total_reward': self.episode_reward,
            'final_accuracy': self._get_classification_accuracy()
        }


# -------------------------------
# 4. Complete ActorCritic Network (ë§ˆìŒ ì—­í• )
# -------------------------------
class ActorCritic(nn.Module):
    def __init__(self, input_shape=(3, 300, 300), num_actions=16, feature_dim=300, 
                 action_space_size=32):
        """
        Complete Actor-Critic for steganalysis with visual attention
        
        Args:
            input_shape: (C, H, W) input image shape
            num_actions: number of movement actions (ë…¼ë¬¸ì—ì„œ 16ê°œ ë°©í–¥)
            feature_dim: feature dimension
            action_space_size: movement step size in pixels
        """
        super(ActorCritic, self).__init__()
        
        
        # Visual attention CNN (ëˆˆ)
        self.attention_cnn = ResNet(in_channels=input_shape[0], 
                                        feature_dim=feature_dim,
                                        mode = 'feature')
        
        # Discriminant model (ë‡Œ)
        self.discriminant = ResNet(in_channels=input_shape[0])
        
        # Actor network (ì •ì±… ë„¤íŠ¸ì›Œí¬)
        self.actor = nn.Sequential(
            nn.Linear(feature_dim + 2, 64),  # feature_dim=300 + 2 for center
            nn.ReLU(),
            nn.Linear(64, num_actions)
        )
        
        # Critic network (ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬)
        self.critic = nn.Sequential(
            nn.Linear(feature_dim + 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Action space configuration
        self.num_actions = num_actions
        self.action_space_size = action_space_size
        self.input_shape = input_shape
        
        # Define 16 movement directions (8 directions * 2 distances)
        angles = np.linspace(0, 2*np.pi, 9)[:-1]  # 8 directions
        self.action_vectors = []
        for angle in angles:
            # Short distance
            dx_short = int(action_space_size * 0.5 * np.cos(angle))
            dy_short = int(action_space_size * 0.5 * np.sin(angle))
            self.action_vectors.append((dy_short, dx_short))
            
            # Long distance  
            dx_long = int(action_space_size * np.cos(angle))
            dy_long = int(action_space_size * np.sin(angle))
            self.action_vectors.append((dy_long, dx_long))
    
    def forward(self, x, center):
        """
        Forward pass
        Args:
            x: input image (B, C, H, W)
            center: current attention center (B, 2) - (y, x) coordinates
        """
        # Extract features using attention CNN
        attention_features = self.attention_cnn(x, center)
        
        # Normalize center coordinates to [-1, 1]
        H, W = x.shape[2], x.shape[3]
        normalized_center = center.float()
        normalized_center[:, 0] = (normalized_center[:, 0] / H) * 2 - 1  # y
        normalized_center[:, 1] = (normalized_center[:, 1] / W) * 2 - 1  # x
        
        # Combine attention features with center information
        combined_features = torch.cat([attention_features, normalized_center], dim=1)
        
        # Actor output (action probabilities)
        action_logits = self.actor(combined_features)
        
        # Critic output (state value)
        state_value = self.critic(combined_features)
        
        return action_logits, state_value.squeeze(-1)
    
    def get_stego_prediction(self, x, center):
        """
        Get steganalysis prediction using discriminant model
        """
        with torch.no_grad():
            # Apply attention mask
            B, C, H, W = x.shape
            attention_masks = []
            
            for b in range(B):
                mask = generate_attention_mask(H, W, center[b])
                attention_masks.append(mask)
            
            masks = torch.stack(attention_masks).to(x.device)
            attended_x = x * masks
            
            # Get classification logits
            logits = self.discriminant(attended_x)  # shape: (B, 1)
            probs = torch.sigmoid(logits)           # shape: (B, 1)
            
            return logits, probs
    
    def move_attention(self, current_center, action, image_shape):
        """
        Move attention center based on action
        Args:
            current_center: (B, 2) current center coordinates
            action: (B,) action indices
            image_shape: (H, W) image dimensions
        """
        H, W = image_shape
        new_center = current_center.clone()
        
        for b in range(len(action)):
            if action[b] < len(self.action_vectors):
                dy, dx = self.action_vectors[action[b]]
                new_y = max(0, min(H-1, current_center[b, 0] + dy))
                new_x = max(0, min(W-1, current_center[b, 1] + dx))
                new_center[b] = torch.tensor([new_y, new_x])
        
        return new_center
    
    def compute_reward(self, x, center, true_labels):
        """
        Compute reward based on steganalysis accuracy and information entropy
        ë…¼ë¬¸ì˜ ë³´ìƒ í•¨ìˆ˜ êµ¬í˜„
        """
        logits, probs = self.get_stego_prediction(x, center)
        
        # Classification accuracy reward
        pred_labels = (logits.squeeze() > 0).long()  # or torch.sigmoid(logits) > 0.5
        accuracy_reward = (pred_labels == true_labels).float()
        
        # Information entropy penalty (lower entropy = higher reward)
        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
        entropy_reward = 1.0 / (1.0 + entropy)  # Inverse entropy
        
        # Combined reward
        total_reward = accuracy_reward + 0.1 * entropy_reward
        
        return total_reward


def apply_adaptive_filter(env, full_data, actorcritic, max_episode_steps=20):
    """
    ê°•í™”í•™ìŠµ ê¸°ë°˜ attention í•„í„°ë§ì„ ì ìš©í•œ cover ì´ë¯¸ì§€ë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
    stego ì´ë¯¸ì§€ëŠ” blurë§Œ ì ìš©
    
    Returns:
        filtered_images: List[torch.Tensor]  # (3, H, W)
        labels: List[int]                    # 0 (cover) or 1 (stego)
        filenames: List[str]                 # ì›ë³¸ íŒŒì¼ ì´ë¦„
    """
    filtered_images = []
    labels = []
    filenames = []

    for i in range(len(full_data)):
        state = env.reset(image_idx=i)
        image_path, label = full_data[i]
        filename = os.path.basename(image_path)

        with torch.no_grad():
            logits = actorcritic.discriminant(env.current_image.to(env.device))
            prob = torch.sigmoid(logits).item()
            pred_label = int(prob > 0.5)

        if pred_label == 1:
            # stegoë¡œ íŒë‹¨ë˜ë©´ blurë§Œ ì ìš©
            blurred_tensor = env.blur(env.current_image)
            filtered_images.append(blurred_tensor.squeeze(0))  # (3, H, W)
            labels.append(0)
            filenames.append(filename)
            continue

        # ê°•í™”í•™ìŠµ ê¸°ë°˜ attention ìˆ˜í–‰
        best_reward = -float('inf')
        best_center = None

        for step in range(max_episode_steps):
            image = state['image'].to(device)
            center = state['center'].to(device)

            with torch.no_grad():
                logits, _ = actorcritic(image, center)
                probs = F.softmax(logits, dim=-1)
                dist = torch.distributions.Categorical(probs)
                action = dist.sample().item()

            next_state, reward, done, info = env.step(action)

            if reward > best_reward:
                best_reward = reward
                best_center = env.current_center.clone()

            if done:
                break

        # best_centerë¡œ attention í•„í„° ì ìš©
        filtered_tensor = env._apply_dual_filter_mask(env.current_image, best_center)
        filtered_images.append(filtered_tensor.squeeze(0))  # (3, H, W)
        labels.append(0)
        filenames.append(filename)

    return filtered_images, labels, filenames

# ê²°ê³¼ ì €ì¥
def save_filtered_data(filtered_imgs, labels, filenames, save_path="adaptive_filtered.pt"):
    data_dict = {
        "images": torch.stack(filtered_imgs),  # (N, 3, H, W)
        "labels": torch.tensor(labels),        # (N,)
        "filenames": filenames                 # List[str], JSON ì§ë ¬í™” ê°€ëŠ¥
    }
    torch.save(data_dict, save_path)
    print(f"[âœ“] í•„í„°ë§ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

# ========================================
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ========================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ìŠ¤í…Œê°€ë¶„ì„ í™˜ê²½ ì´ˆê¸°í™” ì¤‘...")
    
    # 1. ResNet ëª¨ë¸ ë¡œë“œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ResNet ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = ResNet().to(device)
    
    # ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    weight_path = "C:/Users/Admin/Desktop/ê¸°ê³„í•™ìŠµ í”„ë¡œì íŠ¸ (201813784 ì†í˜•ì˜¤)/resnet_trained.pth"
    if os.path.exists(weight_path):
        try:
            model.load_state_dict(torch.load(weight_path, map_location=device, weights_only=False))
            print("ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        print(f"ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weight_path}")
        print("ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # model.train()
    
    # 2. ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
    print("ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")

    # ========================================
    # 3. ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜
    # ========================================

    def make_dataset(cover_dir, stego_dir):
        cover_paths = [os.path.join(cover_dir, f) for f in os.listdir(cover_dir) if f.endswith('.png')]
        stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        cover_labeled = [(path, 0) for path in cover_paths]
        stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = cover_labeled + stego_labeled

        return full_data
    
    def make_dataset_cover(cover_dir):
        cover_paths = [os.path.join(cover_dir, f) for f in os.listdir(cover_dir) if f.endswith('.png')]
        # stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        cover_labeled = [(path, 0) for path in cover_paths]
        # stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = cover_labeled

        return full_data
    
    def make_dataset_stego(stego_dir):
        stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = stego_labeled

        return full_data
    
    # 3. transform ì„¤ì •
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    cover_dir = "C:/Users/Admin/Desktop/Image_data/Original/train"
    stego_dir = "C:/Users/Admin/Desktop/Image_data/Stegno/stg_class"
    full_data = make_dataset(cover_dir, stego_dir)

    # ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    images = [transform(Image.open(path).convert("RGB")) for path, _ in full_data]
    labels = [torch.tensor(label, dtype=torch.long) for _, label in full_data]
        
    print(f"ì´ë¯¸ì§€ ê°œìˆ˜: {len(images)}")
    print(f"ë¼ë²¨ ê°œìˆ˜: {len(labels)}")

    # 3. í™˜ê²½ ìƒì„±
    print("í™˜ê²½ ìƒì„± ì¤‘...")

    # ---------------------------------------------
    # ğŸ”½ [1] ActorCritic ëª¨ë¸ ì„ ì–¸
    actor_critic_model = ActorCritic(input_shape=(3, 300, 300), num_actions=16).to(device)

    env = SteganalysisEnv(
        image_dataset=images,
        labels_dataset=labels,
        discriminant_model=model,
        max_episode_steps=20
    )

    trainer = PPOTrainer(model=actor_critic_model, env=env, device=device)

    # 4. í™˜ê²½ í…ŒìŠ¤íŠ¸
    print("í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    state = env.reset()
    print(f"ì´ˆê¸° ì¤‘ì‹¬ì : {state['center'].cpu().numpy()}")
    
    torch.autograd.set_detect_anomaly(False)
    
    # ğŸ”½ [2] ì €ì¥ëœ ActorCritic ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    actorcritic_path = "C:/Users/Admin/Desktop/ActorCritic_trained.pth"

    # ---------------------------------------------
    # ğŸ”½ [3] PPO í•™ìŠµ ìˆ˜í–‰
    trainer.train(epochs=10, num_rollout_episodes=20)
    
    # ğŸ”½ [4] í•™ìŠµëœ ActorCritic ëª¨ë¸ ì €ì¥
    torch.save(actor_critic_model.state_dict(), actorcritic_path)
    print(f"[âœ“] í•™ìŠµëœ ActorCritic ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {actorcritic_path}")

    # 5. ì‹œê°í™”
    print("ê²°ê³¼ ì‹œê°í™”...")
    env.render("attention_vis.png")
    print("ì‹œê°í™” ê²°ê³¼ê°€ 'attention_vis.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    

if __name__ == "__main__":
    total_start = time.time()
    # main()

    # ==========================
    # 1. ActorCritic ëª¨ë¸ ì„ ì–¸
    # ==========================
    ac_test = ActorCritic(input_shape=(3, 300, 300), num_actions=16).to(device)

    # ==========================
    # 2. ì €ì¥ëœ pth íŒŒì¼ ë¡œë“œ
    # ==========================
    load_path = "C:/Users/Admin/Desktop/ActorCritic_trained.pth"

    if os.path.exists(load_path):
        ac_test.load_state_dict(torch.load(load_path, map_location=device))
        ac_test.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        print(f"[âœ“] ì €ì¥ëœ ActorCritic ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {load_path}")
    else:
        print(f"[!] ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {load_path}")

    def make_dataset(cover_dir, stego_dir):
        cover_paths = [os.path.join(cover_dir, f) for f in os.listdir(cover_dir) if f.endswith('.png')]
        stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        cover_labeled = [(path, 0) for path in cover_paths]
        stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = cover_labeled + stego_labeled

        return full_data
    
    def make_dataset_cover(cover_dir):
        cover_paths = [os.path.join(cover_dir, f) for f in os.listdir(cover_dir) if f.endswith('.png')]
        # stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        cover_labeled = [(path, 0) for path in cover_paths]
        # stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = cover_labeled

        return full_data
    
    def make_dataset_stego(stego_dir):
        stego_paths = [os.path.join(stego_dir, f) for f in os.listdir(stego_dir) if f.endswith('.png')]

        # ë¼ë²¨ë§
        stego_labeled = [(path, 1) for path in stego_paths]

        # Cover + Stego í•©ì¹˜ê³  ì…”í”Œ
        full_data = stego_labeled

        return full_data
    
    # 3. transform ì„¤ì •
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

        # ResNet ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = ResNet().to(device)
    
    # ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    weight_path = "C:/Users/Admin/Desktop/ê¸°ê³„í•™ìŠµ í”„ë¡œì íŠ¸ (201813784 ì†í˜•ì˜¤)/resnet_trained.pth"
    if os.path.exists(weight_path):
        try:
            model.load_state_dict(torch.load(weight_path, map_location=device, weights_only=False))
            print("ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        print(f"ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weight_path}")
        print("ëœë¤ ê°€ì¤‘ì¹˜ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    # í…ŒìŠ¤íŠ¸
    test_cover_dir = "C:/Users/Admin/Desktop/Image_data/Test/cover"
    test_stego_dir = "C:/Users/Admin/Desktop/Image_data/Test/stego"

    test_data = make_dataset_cover(test_cover_dir)

    # ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    images = [transform(Image.open(path).convert("RGB")) for path, _ in test_data]
    labels = [torch.tensor(label, dtype=torch.long) for _, label in test_data]
        
    print(f"ì´ë¯¸ì§€ ê°œìˆ˜: {len(images)}")
    print(f"ë¼ë²¨ ê°œìˆ˜: {len(labels)}")

    # 3. í™˜ê²½ ìƒì„±
    print("í™˜ê²½ ìƒì„± ì¤‘...")

    env = SteganalysisEnv(
        image_dataset=images,
        labels_dataset=labels,
        discriminant_model=model,
        max_episode_steps=20
    )

    print("Best attention ìœ„ì¹˜ íƒìƒ‰...")
    start_time = time.time()
    filtered_imgs, labels, filenames = apply_adaptive_filter(env, test_data, ac_test)
    save_filtered_data(filtered_imgs, labels, filenames, save_path="C:/Users/Admin/Desktop/Python/Attention_Disc/adaptive_test_cover.pt")

    

    end_time = time.time()

    total_elapsed = end_time - total_start
    eval_elapsed = end_time - start_time

    print(f"\nğŸ•’ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
    print(f"\nğŸ•’ í‰ê°€ ì‹¤í–‰ ì‹œê°„: {eval_elapsed:.2f}ì´ˆ")
